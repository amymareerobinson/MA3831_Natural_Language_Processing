{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "collapsed_sections": [
    "ivfvXWlK41Ji",
    "wbHzKwfc4ulX"
   ],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dI93ABlBZKB"
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import WordNetLemmatizer\n",
    "import gensim\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from keras import Sequential\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, Dense, LSTM, Conv1D, Embedding, SpatialDropout1D\n",
    "from sklearn.metrics import confusion_matrix, classification_report, multilabel_confusion_matrix\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from datetime import datetime\n",
    "import re\n",
    "import unicodedata\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "id": "Czbvt9-kC_dr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive"
   ],
   "metadata": {
    "id": "ChygquzQBeie"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Web Crawler\n",
    "\n"
   ],
   "metadata": {
    "id": "ivfvXWlK41Ji"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Science News Explores\n"
   ],
   "metadata": {
    "id": "apjsBExheFBR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "links = []; titles = []; authors = []; dates = []; contents = []\n",
    "\n",
    "url = 'https://www.snexplores.org/topic/environment'\n",
    "page_url = '/page/'\n",
    "pages = 28\n",
    "\n",
    "for page in np.arange(1, pages+1):\n",
    "      request = requests.get(url + page_url + str(page))\n",
    "      soup = BeautifulSoup(request.text, \"html.parser\")\n",
    "      h3href = soup.find_all(\"h3\", {\"class\":\"post-item-river__title___OLWRU\"})\n",
    "      for i in h3href:\n",
    "            ahref = i.find('a').get('href')\n",
    "            request_link = requests.get(ahref)\n",
    "            link = request_link.url\n",
    "            links.append(link)\n",
    "            print(link)\n",
    "\n",
    "            article = requests.get(link)\n",
    "            article_content = article.content\n",
    "            soup_article = BeautifulSoup(article_content, \"html.parser\")\n",
    "\n",
    "            # title\n",
    "            title = soup_article.find('title').get_text()\n",
    "            titles.append(title)\n",
    "\n",
    "            # author\n",
    "            author = soup_article.find('span', {'class':'byline author vcard'}).get_text()\n",
    "            authors.append(author)\n",
    "\n",
    "            # date\n",
    "            date = soup_article.find('time', {'class':'date entry-date byline__published___LoyMY'}).attrs['datetime'][:10]\n",
    "            dates.append(date)\n",
    "\n",
    "            # content\n",
    "            body = soup_article.find('div', {\"class\":\"rich-text rich-text--with-sidebar single__rich-text___DT62t\"})\n",
    "            content = body.find_all('p', { \"class\" : \"\" })\n",
    "\n",
    "            advert = body.find('div', class_='newsletter-signup__message___pemaq')\n",
    "            if advert is not None:\n",
    "                  body.find('div', class_='newsletter-signup__message___pemaq').decompose()\n",
    "\n",
    "            paragraphs = []\n",
    "            for p in content:\n",
    "                  paragraph = p.get_text() # string\n",
    "                  paragraphs.append(paragraph)\n",
    "                  final = \" \".join(paragraphs)\n",
    "            contents.append(final)\n",
    "\n",
    "print(\"DONE!\")"
   ],
   "metadata": {
    "id": "7Av9lge_45NW",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "outputId": "d71a1850-73d5-4d26-85df-42d364ba969a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset1 = pd.DataFrame({'Title':titles, 'Author':authors, 'Date':dates, 'Link':links, 'Content':contents}); print(f'length: {len(dataset1)}') # 328"
   ],
   "metadata": {
    "id": "a83hXoSdd2ux"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Science News Independent\n"
   ],
   "metadata": {
    "id": "7rNoQ9sceJiw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "links = []; titles = []; authors = []; dates = []; contents = []\n",
    "\n",
    "url = 'https://www.sciencenews.org/topic/environment'\n",
    "page_url = '/page/'\n",
    "pages = 32\n",
    "\n",
    "for page in np.arange(1, pages+1):\n",
    "      request = requests.get(url + page_url + str(page))\n",
    "      soup = BeautifulSoup(request.text, \"html.parser\")\n",
    "      h3href = soup.find_all(\"h3\", {\"class\":\"post-item-river__title___vyz1w\"})\n",
    "      for i in h3href:\n",
    "            ahref = i.find('a').get('href')\n",
    "            request_link = requests.get(ahref)\n",
    "            link = request_link.url\n",
    "            links.append(link)\n",
    "            print(link)\n",
    "\n",
    "            article = requests.get(link)\n",
    "            article_content = article.content\n",
    "            soup_article = BeautifulSoup(article_content, \"html.parser\")\n",
    "\n",
    "            # title\n",
    "            keep = soup_article.find('div', {'class':'header-default__terms___-BVta'})\n",
    "            if keep is not None:\n",
    "                  title = soup_article.find('h1', {'class':'header-default__title___ychM4'}).get_text()\n",
    "                  titles.append(title)\n",
    "            else:\n",
    "                  title = soup_article.find('h1', {'class':'header-immersive__title___asgYs'}).get_text()\n",
    "                  titles.append(title)\n",
    "\n",
    "            # author\n",
    "            author = soup_article.find('span', {'class':'byline author vcard'}).get_text()\n",
    "            authors.append(author)\n",
    "\n",
    "            # date\n",
    "            date = soup_article.find('time', {'class':'byline__published___LoyMY'}).attrs['datetime'][:10]\n",
    "            dates.append(date)\n",
    "\n",
    "            # content\n",
    "            body = soup_article.find('div', {\"class\":\"rich-text rich-text--with-sidebar single__rich-text___RmCDp\"})\n",
    "            content = body.find_all('p', { \"class\" : \"\" })\n",
    "\n",
    "            paragraphs = []\n",
    "            for p in content:\n",
    "                  paragraph = p.get_text()\n",
    "                  paragraphs.append(paragraph)\n",
    "                  final = \" \".join(paragraphs)\n",
    "            contents.append(final)\n",
    "\n",
    "print(\"DONE!\")"
   ],
   "metadata": {
    "id": "URYtEtP6d20Z"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset2 = pd.DataFrame({'Title':titles, 'Author':authors, 'Date':dates, 'Link':links, 'Content':contents}); print(f'length: {len(dataset2)}') # 376"
   ],
   "metadata": {
    "id": "P9IIjz6nePG5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Frontiers Science News\n"
   ],
   "metadata": {
    "id": "pCN_6olfeM14"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "links = []; titles = []; authors = []; dates = []; contents = []\n",
    "\n",
    "url = 'https://blog.frontiersin.org/category/environment/'\n",
    "page_url = '/page/'\n",
    "pages = 3\n",
    "\n",
    "for page in np.arange(1, pages+1):\n",
    "      url = 'https://blog.frontiersin.org/category/environment/'\n",
    "      request = requests.get(url + page_url + str(page))\n",
    "      soup = BeautifulSoup(request.text, \"html.parser\")\n",
    "      h3href = soup.find_all(\"h3\", {\"class\":\"loop-title\"})\n",
    "      for i in h3href:\n",
    "            ahref = i.find('a').get('href')\n",
    "            request_link = requests.get(ahref)\n",
    "            link = request_link.url\n",
    "            links.append(link)\n",
    "            print(link)\n",
    "\n",
    "            article = requests.get(link)\n",
    "            article_content = article.content\n",
    "            soup_article = BeautifulSoup(article_content, \"html.parser\")\n",
    "\n",
    "            # title\n",
    "            title = i.get_text()\n",
    "            titles.append(title)\n",
    "\n",
    "            # author\n",
    "            author = soup_article.find('div', {'class':'entry clearfix'}).find('p').get_text().split(',')[0][3:]\n",
    "            authors.append(author)\n",
    "\n",
    "            # date\n",
    "            date = soup_article.find('span', {'class':'updated'}).get_text()\n",
    "            date = datetime.strptime(date, '%B %d, %Y').date()\n",
    "            dates.append(date)\n",
    "\n",
    "            # content\n",
    "            body = soup_article.find('div', {\"class\":\"entry clearfix\"})\n",
    "            if soup_article.find('hr') in body:\n",
    "                  body.find('hr').find_next('a').decompose()\n",
    "                  body.find('hr').find_next().find_next('a').decompose()\n",
    "            body.find_next('p').decompose()\n",
    "            content = body.find_all('p', { \"class\" : \"\" })\n",
    "            content[-1].decompose()\n",
    "\n",
    "            paragraphs = []\n",
    "            for p in content:\n",
    "                  paragraph = p.get_text()\n",
    "                  paragraphs.append(paragraph)\n",
    "                  final = \" \".join(paragraphs)\n",
    "            contents.append(final)\n",
    "\n",
    "print(\"DONE!\")"
   ],
   "metadata": {
    "id": "wsQ8iYaHd23y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset3 = pd.DataFrame({'Title':titles, 'Author':authors, 'Date':dates, 'Link':links, 'Content':contents}); print(f'length: {len(dataset3)}') # 108"
   ],
   "metadata": {
    "id": "i-aLmJukeR0L"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The New Daily\n"
   ],
   "metadata": {
    "id": "ieNCIFnteR83"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "links = []; titles = []; authors = []; dates = []; contents = []\n",
    "\n",
    "url = 'https://thenewdaily.com.au/life/science/environment'\n",
    "page_url = '/page/'\n",
    "pages = 50\n",
    "\n",
    "for page in np.arange(0, pages+1):\n",
    "      url = 'https://thenewdaily.com.au/life/science/environment'\n",
    "      request = requests.get(url + page_url + str(page))\n",
    "      soup = BeautifulSoup(request.text, \"html.parser\")\n",
    "      h3href = soup.find_all(\"a\", {\"class\":\"tnd-thumb__link\"})\n",
    "      for i in h3href:\n",
    "            ahref = i.attrs['href']\n",
    "            request_link = requests.get(ahref)\n",
    "            link = request_link.url\n",
    "            links.append(link)\n",
    "            print(link)\n",
    "\n",
    "            article = requests.get(link)\n",
    "            article_content = article.content\n",
    "            soup_article = BeautifulSoup(article_content, \"html.parser\")\n",
    "\n",
    "            # title\n",
    "            title = i.find(\"h1\", {'class':'tnd-thumb__h1'}).get_text()\n",
    "            titles.append(title)\n",
    "\n",
    "            # author\n",
    "            author = soup_article.find('span', {'itemprop':'name'}).get_text()\n",
    "            authors.append(author)\n",
    "\n",
    "            # date\n",
    "            date = soup_article.find(\"time\", {'class':'timeago tnd-time'})\n",
    "            if date is None:\n",
    "                  date = soup_article.find(\"time\", {'class':'tnd-time'})\n",
    "                  if date is None:\n",
    "                        date = soup_article.find(\"time\", {'class':'article-hero__time'})\n",
    "            date = date.attrs['datetime'][:10]\n",
    "            dates.append(date)\n",
    "\n",
    "            # content\n",
    "            body = soup_article.find('div', {\"class\":\"tnd-content-style tnd-content-style--article\"})\n",
    "            if body.find(\"p\").find(\"strong\") in body:\n",
    "                  print('yes')\n",
    "            content = body.find_all('p')\n",
    "            content[-1].decompose()\n",
    "\n",
    "            paragraphs = []\n",
    "            for p in content:\n",
    "                  paragraph = p.get_text()\n",
    "                  paragraphs.append(paragraph)\n",
    "                  final = \" \".join(paragraphs)\n",
    "            contents.append(final)\n",
    "\n",
    "print(\"DONE!\")"
   ],
   "metadata": {
    "id": "iotKwMwPeVs2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset4 = pd.DataFrame({'Title':titles, 'Author':authors, 'Date':dates, 'Link':links, 'Content':contents}); print(f'length: {len(dataset4)}') #510"
   ],
   "metadata": {
    "id": "_3r6A-6eeZNw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = pd.concat([dataset1, dataset2, dataset3, dataset4], ignore_index=True); print(f'length: {len(dataset)}') #1322"
   ],
   "metadata": {
    "id": "RpFjT-iWeZXh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset.to_excel(\"MA3831_TP1_2023_A3_Dataset_output.xlsx\")"
   ],
   "metadata": {
    "id": "EG-wG1QRed-c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Wrangling and Word Embedding\n"
   ],
   "metadata": {
    "id": "wbHzKwfc4ulX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#@title\n",
    "drive.mount('/content/gdrive')\n",
    "dataset = pd.read_excel(r'/content/gdrive/My Drive/Colab Notebooks/MA3831_TP1_2023_Dataset_output.xlsx')\n",
    "dataset = dataset.iloc[:, 1:] # drop first column from the dataset\n",
    "\n",
    "print(f\"length before: {len(dataset)}\")\n",
    "dataset = dataset.dropna()\n",
    "print(f\"length after: {len(dataset)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T75nT1yaBeoi",
    "outputId": "da3de5a3-ae0e-4528-ea69-c4faf5ff8c89",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset.drop_duplicates(subset=['Title','Author'],keep='first',inplace=True)\n",
    "\n",
    "dataset['Date'] = dataset['Date'].apply(str).str[:7].str.replace('-', '', regex=True).astype(int)\n",
    "dataset['content_words'] = dataset['Content'].str.split().str.len()\n",
    "dataset['title_words'] = dataset['Title'].str.split().str.len()\n",
    "\n",
    "dataset.drop(dataset[dataset['content_words'] < 200].index, inplace=True)\n",
    "dataset.drop(dataset[dataset['title_words'] < 6].index, inplace=True)\n"
   ],
   "metadata": {
    "id": "SSDt9DYzBerO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset['Sentiment'] = dataset['Sentiment'].astype(str)\n",
    "dataset['Sentiment'] = dataset['Sentiment'].str.replace('-1', '0').astype(int)\n",
    "print(\"Number of rows per sentiment rating:\")\n",
    "print(dataset['Sentiment'].value_counts())\n",
    "ax = dataset.groupby('Sentiment').count().plot(kind='bar', legend=False, color='black')\n",
    "ax = ax.set_xticklabels(['Negative','Positive'], rotation=0)\n",
    "\n",
    "# Function to retrieve top few number of each category\n",
    "top_n = 500\n",
    "\n",
    "def get_top_data(top_n):\n",
    "    top_data_df_positive = dataset[dataset['Sentiment'] == 1].head(top_n)\n",
    "    top_data_df_negative = dataset[dataset['Sentiment'] == 0].head(top_n)\n",
    "    top_data_df_small = pd.concat([top_data_df_positive, top_data_df_negative])\n",
    "    return top_data_df_small\n",
    "\n",
    "# Function call to get the top 10000 from each sentiment\n",
    "dataset = get_top_data(top_n=top_n)\n",
    "\n",
    "# After selecting top few samples of each sentiment\n",
    "print(\"After segregating and taking equal number of rows for each sentiment:\")\n",
    "print(dataset['Sentiment'].value_counts())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "7p4ZjU5CyAQw",
    "outputId": "8f26901c-a4a3-4bfa-a24a-c30ce5086f29"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Normalising Text\n",
    "def normalise_text(string_text):\n",
    "  if isinstance(string_text, str):\n",
    "    #replace lower format\n",
    "    string_text = string_text.lower()\n",
    "    #remove special characters\n",
    "    pattern = r'''(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?|(?:[A-Z]\\.)|\\$?\\d+(?:\\.\\d+)?%?|\\.\\.\\.| [][.,;\"'?():_'-]'''\n",
    "    string_text = re.sub(pattern, ' ', string_text)\n",
    "    #remove accented characters\n",
    "    string_text = unicodedata.normalize('NFKD', string_text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    #remove stopwords\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')\n",
    "    tokenizer = ToktokTokenizer()\n",
    "    tokens = tokenizer.tokenize(string_text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    string_text = ' '.join(filtered_tokens)\n",
    "    return string_text\n",
    "\n",
    "\n",
    "def lemmatize_text(string_text):\n",
    "  wnl = WordNetLemmatizer()\n",
    "  string_text = nltk.word_tokenize(string_text)\n",
    "  string_text = ' '.join(wnl.lemmatize(token) for token in string_text)\n",
    "  return string_text"
   ],
   "metadata": {
    "id": "djGaVEAMBeuy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "features = ['Title', 'Author', 'Content']\n",
    "\n",
    "for feature in features:\n",
    "  dataset[feature] = dataset[feature].apply(lemmatize_text)\n",
    "  dataset[feature] = dataset[feature].apply(normalise_text)\n",
    "  dataset[feature] = dataset[feature].apply(gensim.utils.simple_preprocess)"
   ],
   "metadata": {
    "id": "RDWeAw4KBr5M"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_topics():\n",
    "    topic_list = []\n",
    "    for title, content in zip(dataset.Title, dataset.Content):\n",
    "      results = {}\n",
    "      for string in title:\n",
    "        results[string] = content.count(string)\n",
    "      string = max(results, key=results.get)\n",
    "      topic_list.append(string)\n",
    "    return topic_list\n",
    "\n",
    "topics = extract_topics()\n",
    "dataset['Topic'] = topics\n",
    "\n",
    "# for text in dataset.Topic:\n",
    "common_topics = Counter(\" \".join(topics).split()).most_common(10)\n",
    "print(common_topics)"
   ],
   "metadata": {
    "id": "qVZG451BBr7l",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "41af4b43-57ff-4c31-bc2c-4438bf093c4c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def wordcloud_plot(topics):\n",
    "  #plot the word cloud object\n",
    "  text = ' '.join(topics)\n",
    "\n",
    "  wordcloud = WordCloud(stopwords = STOPWORDS, collocations=True, min_word_length=5).generate(text)\n",
    "  plt.imshow(wordcloud, interpolation='bilInear')\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "\n",
    "  # create a dictionary of word frequencies\n",
    "  text_dictionary = wordcloud.process_text(text)\n",
    "  # sort the dictionary\n",
    "  word_freq={k: v for k, v in sorted(text_dictionary.items(),reverse=True, key=lambda item: item[1])}\n",
    "  #use words_ to print relative word frequencies\n",
    "  rel_freq = wordcloud.words_\n",
    "  #print results\n",
    "  print(list(word_freq.items())[:15])\n",
    "  print(list(rel_freq.items())[:10])"
   ],
   "metadata": {
    "id": "IhHx2s1b5bLy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#negative word cloud\n",
    "negative_topics = dataset[dataset.Sentiment == 0]\n",
    "negative_topics = negative_topics['Topic']\n",
    "\n",
    "wordcloud_plot(negative_topics)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "7UsgFBB8lV6I",
    "outputId": "2893ceb4-770f-4636-ab53-d5505545a6bf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#positive word cloud\n",
    "positive_topics = dataset[dataset.Sentiment == 1]\n",
    "positive_topics = positive_topics['Topic']\n",
    "\n",
    "wordcloud_plot(positive_topics)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "0VYBZBoIfh64",
    "outputId": "e919dc45-594b-4c3e-adf4-dc743fb33056"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#training and testing split data\n",
    "X_data, y_data = np.array(dataset['Content']), np.array(dataset['Sentiment'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.05, random_state = 0)"
   ],
   "metadata": {
    "id": "5wP3k-T7fYIK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "Word2vec_train_data = list(map(lambda x: x, X_train))\n",
    "\n",
    "Embedding_dimensions = 500 \n",
    "window = 5\n",
    "\n",
    "word2vec_model  = Word2Vec(Word2vec_train_data,\n",
    "                     min_count=1,\n",
    "                     vector_size=Embedding_dimensions,\n",
    "                     window=window,\n",
    "                     sg=0)\n",
    "\n",
    "print(\"Vocabulary Length:\", len(word2vec_model.wv.key_to_index))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yQ2xDfMtfYNd",
    "outputId": "5960c7fa-b6aa-4b76-e947-19ad281ae925"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(word2vec_model.wv.most_similar(positive=['climate'], topn=3))"
   ],
   "metadata": {
    "id": "Myo9BRKsDJpK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3ef2252c-f596-49db-9062-c42db85f8122"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "keys = ['climate', 'plastic', 'water', 'air', 'oil']\n",
    "\n",
    "embedding_clusters = []\n",
    "word_clusters = []\n",
    "for word in keys:\n",
    "    embeddings = []\n",
    "    words = []\n",
    "    for similar_word, words_ in word2vec_model.wv.most_similar(word, topn=30):\n",
    "        words.append(similar_word)\n",
    "        embeddings.append(word2vec_model.wv[similar_word])\n",
    "    embedding_clusters.append(embeddings)\n",
    "    word_clusters.append(words)\n",
    "\n",
    "tsne_model_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
    "embedding_clusters = np.array(embedding_clusters)\n",
    "n, m, k = embedding_clusters.shape\n",
    "embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
    "\n",
    "\n",
    "def tsne_plot_similar_words(labels, embedding_clusters, word_clusters, a=0.7):\n",
    "    plt.figure(figsize=(19, 9))\n",
    "    # colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
    "    colors = ['green', 'orange', 'blue', 'red', 'black']\n",
    "    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n",
    "        x = embeddings[:,0]\n",
    "        y = embeddings[:,1]\n",
    "        plt.scatter(x, y, c=color, alpha=a, label=label)\n",
    "        for i, word in enumerate(words):\n",
    "            plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 5),\n",
    "                         textcoords='offset points', ha='right', va='bottom', size=9)\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "tsne_plot_similar_words(keys, embeddings_en_2d, word_clusters)\n",
    "\n",
    "for word in keys:\n",
    "    positive_words = word2vec_model.wv.most_similar(positive=[word], topn=3)\n",
    "    negative_words = word2vec_model.wv.most_similar(negative=[word], topn=3)\n",
    "\n",
    "    print(f\"{word}: {positive_words} \\n\")"
   ],
   "metadata": {
    "id": "U2yQf4swDaa9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "outputId": "32d031c2-3f89-4d62-841a-6fe75a1d5dd1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Deep Learning"
   ],
   "metadata": {
    "id": "XDRU9JZ5UO4D"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Defining the model input length.\n",
    "input_length = 1000\n",
    "vocab_length = 30000\n",
    "\n",
    "tokenizer = Tokenizer(filters=\"\", lower=False, oov_token=\"<oov>\")\n",
    "tokenizer.fit_on_texts(X_data)\n",
    "tokenizer.num_words = vocab_length\n",
    "print(\"Tokenizer vocab length:\", vocab_length)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nAhKGCehfYP-",
    "outputId": "6e0f7451-d9a7-4612-dce2-25abea53d748"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=input_length)\n",
    "X_test  = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=input_length)"
   ],
   "metadata": {
    "id": "ZfVQhvZ7fYSb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "embedding_matrix = np.zeros((vocab_length, Embedding_dimensions))\n",
    "\n",
    "for word, token in tokenizer.word_index.items():\n",
    "    if word2vec_model.wv.__contains__(word):\n",
    "      embedding_matrix[token] = word2vec_model.wv.__getitem__(word)"
   ],
   "metadata": {
    "id": "XL5nWW_kBr-E"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def getModel():\n",
    "    embedding_layer = Embedding(input_dim = vocab_length,\n",
    "                                output_dim = Embedding_dimensions,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=input_length,\n",
    "                                trainable=False)\n",
    "\n",
    "    model = Sequential([\n",
    "        embedding_layer,\n",
    "        Bidirectional(LSTM(200, dropout=0.5, return_sequences=True)),\n",
    "        Bidirectional(LSTM(150, dropout=0.2, return_sequences=True)),\n",
    "        GlobalMaxPool1D(),\n",
    "        Dense(24, activation='relu'),\n",
    "        Dense(1, activation='sigmoid'),\n",
    "    ],\n",
    "    name=\"Sentiment_Model\")\n",
    "    return model"
   ],
   "metadata": {
    "id": "v1CRvvHP-63f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "training_model = getModel()\n",
    "training_model.summary()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GEFfiuGZBsC6",
    "outputId": "cce58762-6be9-447b-c84e-43b6b687d36a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n",
    "             EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]\n",
    "\n",
    "training_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])         "
   ],
   "metadata": {
    "id": "YgS6hSVi2y6Y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "history = training_model.fit(\n",
    "    X_train, y_train,         \n",
    "    batch_size=300,\n",
    "    # batch_size=256, # 0.69 (winner!)\n",
    "    epochs=50,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4FKTXP671fB",
    "outputId": "bdcc8fff-abf2-4e4e-b13f-4519cdba8097"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# acc,  val_acc  = history.history['accuracy'], history.history['val_accuracy']\n",
    "# loss, val_loss = history.history['loss'], history.history['val_loss']\n",
    "# epochs = range(len(acc))\n",
    "\n",
    "\n",
    "# plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "# plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "# plt.title('Training and validation accuracy')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.figure()\n",
    "\n",
    "# plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "# plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.title('model loss vs accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['loss', 'accuracy'], loc='upper right')\n",
    "# plt.show() "
   ],
   "metadata": {
    "id": "6UeT1d9E2zBB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def ConfusionMatrix(y_pred, y_test):\n",
    "    # Compute and plot the Confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    categories  = ['Negative','Positive']\n",
    "    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
    "\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n",
    "                xticklabels = categories, yticklabels = categories)\n",
    "\n",
    "    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)"
   ],
   "metadata": {
    "id": "jTWEo0iY2zIF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Predicting on the Test dataset.\n",
    "y_pred = training_model.predict(X_test)\n",
    "\n",
    "# Converting prediction to reflect the sentiment predicted.\n",
    "y_pred = np.where(y_pred>=0.5, 1, 0)\n",
    "\n",
    "# Printing out the Evaluation metrics. \n",
    "ConfusionMatrix(y_pred, y_test)"
   ],
   "metadata": {
    "id": "Ji080fMsZ1O5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "outputId": "dd1b54bc-d83d-48c6-a969-eb5c32d22460"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
    "labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]"
   ],
   "metadata": {
    "id": "8HbjGqs3vqjC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Print the evaluation metrics for the dataset.\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "id": "5JhQjHN93tfA",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ccc8d5cb-c572-4d2d-f943-9319c8ef9e44"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "keys = ['climate', 'plastic', 'water', 'air', 'oil']\n",
    "df = dataset[(dataset.Year >= 2022) & (dataset['Topic'].isin(keys))]"
   ],
   "metadata": {
    "id": "QDeyOU7FzUHU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sentiment_list = []\n",
    "for data in df['Content']:\n",
    "  twt = data\n",
    "  #vectorizing the tweet by the pre-fitted tokenizer instance\n",
    "  twt = tokenizer.texts_to_sequences(twt)\n",
    "  #padding the tweet to have exactly the same shape as `embedding_2` input\n",
    "  twt = pad_sequences(twt, maxlen=500, dtype='int32', value=0)\n",
    "  sentiment = training_model.predict(twt,batch_size=1,verbose = 2)\n",
    "  print(np.average(sentiment))\n",
    "  if (np.average(sentiment) <= 0.5):\n",
    "    predict_sentiment = 0\n",
    "  else:\n",
    "    predict_sentiment = 1\n",
    "  sentiment_list.append(predict_sentiment)\n",
    "\n",
    "df['predicted_sentiment'] = sentiment_list"
   ],
   "metadata": {
    "id": "avva31nCry1c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cross tabulation between Topic and predicted_sentiment\n",
    "CrosstabResult=pd.crosstab(index=df['Topic'],columns=df['predicted_sentiment'])\n",
    "print(CrosstabResult)\n",
    " \n",
    "# Grouped bar chart between Topic and predicted_sentiment\n",
    "CrosstabResult.plot.bar(figsize=(7,4), rot=0, color={0:'grey',1:'black'})\n",
    "plt.legend([\"Negative\",\"Positive\"])"
   ],
   "metadata": {
    "id": "uppNlzsvehx0"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
